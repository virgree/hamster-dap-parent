# canal配置
canal.server.ip=node1
canal.server.port=11111
canal.server.destination=example
canal.server.username=canal
canal.server.password=canal
canal.subscribe.filter=testdb.*

# zookeeper配置
zookeeper.server.ip=node1:2181,node2:2181,node3:2181

# kafka配置
# kafka集群地址
kafka.bootstrap_servers_config=node1:9092,node2:9092,node3:9092
# 配置批次发送数据的大小，满足批次大小才会发送数据
kafka.batch_size_config=1024
# 1：表示leader节点写入成功，就返回，假设leader节点写入成功以后没有来得及同步，宕机了，数据会丢失
# 0：异步操作，不管有没有写入成功，都返回，存在丢失数据的可能
# -1：当leader节点写入成功，同时从节点同步成功以后才会返回，可以保证数据的不丢失
kafka.acks=all
# 重试次数
kafka.retries=0
kafka.client_id_config=testdb_canal_01
# kafka的key序列化
kafka.key_serializer_class_config=org.apache.kafka.common.serialization.StringSerializer
# kafka的value序列化，这个序列化方式是需要自定义开发的
kafka.value_serializer_class_config=com.google.dap.canal.protobuf.ProtoBufSerializer
# 数据写入到kafka的哪个topic中
kafka.topic=ods_testdb_mysql